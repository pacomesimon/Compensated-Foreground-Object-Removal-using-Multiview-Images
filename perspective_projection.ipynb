{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PZsNvr9Tf6bi"},"outputs":[],"source":["\n","import numpy as np\n","import os\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import cv2\n","import torch\n","import torchvision\n","from torchvision import transforms as T\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26554,"status":"ok","timestamp":1714025995187,"user":{"displayName":"Pacome Simon Mbonimpa","userId":"08747643083832559810"},"user_tz":-120},"id":"nkSjLuxu9Fd_","outputId":"4592a56d-b850-45a9-b49f-174c1198c78b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1714026310988,"user":{"displayName":"Pacome Simon Mbonimpa","userId":"08747643083832559810"},"user_tz":-120},"id":"TgAn5Zmz1N28","outputId":"bcc57444-863e-4add-da5f-58cb28c77c36"},"outputs":[{"name":"stdout","output_type":"stream","text":["Device:  cpu\n"]}],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", device)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import config as global_config"]},{"cell_type":"markdown","metadata":{"id":"BeBiT_VthS6T"},"source":["# Working on Google streetview images\n","In this notebook, we are going to show the proof of concept of the idea: Compensated Foreground Object removal using Multiview Images"]},{"cell_type":"markdown","metadata":{"id":"ozkPOcqFgYRW"},"source":["## Segmentation using mask RCNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rh0paWWM8L1g"},"outputs":[],"source":["DATA_DIR_PATH = global_config.IMAGES_FILEPATH_PARENT_DIR\n","MASKS_DIR_PATH = global_config.MODEL_MASKS_IMGS_PATH\n","IMAGES_FILEPATH_DIR = global_config.IMAGES_FILEPATH_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RAgNUg_YxQq"},"outputs":[],"source":["STICHED_IMAGES_PATH = global_config.STITCHED_IMGS_PATH"]},{"cell_type":"markdown","metadata":{"id":"kSbOueaJr06_"},"source":["## Perspective projection, using a bounding box (for inpainting)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49508,"status":"ok","timestamp":1714028906045,"user":{"displayName":"Pacome Simon Mbonimpa","userId":"08747643083832559810"},"user_tz":-120},"id":"ZO-ovRNo6F4-","outputId":"1f531008-acd8-4761-98eb-a96e74c938a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["file_names: ['2.jpg', '3.jpg', '4.jpg', '5.jpg', '6.jpg', '7.jpg', '8.jpg', '9.jpg', '10.jpg', '11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg', '16.jpg', '17.jpg', '18.jpg', '19.jpg', '1.jpg', '0.jpg']\n","paths: ['/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/0.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/1.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/2.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/3.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/4.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/5.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/6.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/7.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/8.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/9.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/10.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/11.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/12.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/13.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/14.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/15.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/16.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/17.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/18.jpg', '/content/drive/MyDrive/ACV_Project/sample_data/DATA/streetviewdataset/19.jpg']\n"]}],"source":["def filenames_to_img_arrays(filenames_list):\n","    video_frames = []\n","    for filename in filenames_list:\n","        video_frame = cv2.imread(filename)\n","        r,c,_ = video_frame.shape\n","        video_frame = cv2.resize(video_frame, (c//4, r//4), interpolation=cv2.INTER_AREA)\n","        video_frames.append( cv2.cvtColor(video_frame, cv2.COLOR_BGR2RGB)) #/ 255.0)\n","    return video_frames\n","\n","# Example usage:\n","def list_files(directory_path):\n","    paths = []\n","    file_names = []\n","    for root, _, files in os.walk(directory_path):\n","      file_names.append(files)\n","    file_names = file_names[-1]\n","    print(\"file_names:\", file_names)\n","    N = len(file_names)\n","    file_type = file_names[0].split(\".\")[-1]\n","    for n in range(N):\n","        full_path = os.path.join(root, f\"{n}.{file_type}\")\n","        paths.append(full_path)\n","    print(\"paths:\",paths)\n","    return paths\n","images_filenames_list = list_files(IMAGES_FILEPATH_DIR)\n","equally_spaced_frames_array = filenames_to_img_arrays(images_filenames_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HSxlXDuvFncy"},"outputs":[],"source":["def get_masks_file_paths(path = MASKS_DIR_PATH):\n","  l = os.listdir(MASKS_DIR_PATH)\n","  l_ = [i for i in l if i.split(\".\")[-1]==\"jpg\"]\n","  paths = [MASKS_DIR_PATH+i for i in l_]\n","  indexes = [int(i.split(\".\")[0]) for i in l_]\n","  return paths, indexes\n","masks_paths, masks_indexes = get_masks_file_paths()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMhMdHuETtNK"},"outputs":[],"source":["def find_bounding_box(mask):\n","    # Find the indices of zero pixels\n","    zero_indices = np.argwhere(mask == 0)\n","\n","    # Get the minimum and maximum coordinates\n","    y1 , x1 = np.min(zero_indices, axis=0)\n","    y2, x2 = np.max(zero_indices, axis=0)\n","\n","    # Return the bounding box coordinates\n","    return [x1, y1, x2, y2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iqGOaPtxSNre"},"outputs":[],"source":["def generate_warped_images(STICHED_IMAGES_PATH = STICHED_IMAGES_PATH):\n","  for idx, image in tqdm(enumerate(equally_spaced_frames_array)):\n","    img1_color = image.copy()# Image to be aligned\n","    if idx in masks_indexes:\n","      mask_path =  masks_paths[masks_indexes.index(idx)]\n","      mask = plt.imread(mask_path)\n","    else:\n","      result = img1_color\n","    img1_color_with_patch_removed = (cv2.bitwise_and(img1_color, img1_color, mask=mask) *1).astype(np.uint8)\n","    img2_color = equally_spaced_frames_array.copy()[idx-1].copy() if idx!=0 else  equally_spaced_frames_array.copy()[1].copy() # Reference image\n","    bounding_box  = find_bounding_box(mask)\n","    bounding_tolerance = 0.01\n","    dH,dW, _ = (np.array(img1_color_with_patch_removed.shape) * bounding_tolerance).astype(int)\n","    bounding_box_with_tolerance = bounding_box + np.array([-dW,-dH,dW,dH])\n","    img1 = cv2.cvtColor(img1_color_with_patch_removed, cv2.COLOR_BGR2GRAY)\n","    img2 = cv2.cvtColor(img2_color, cv2.COLOR_BGR2GRAY)\n","    bounding_large_tolerance = 0.1\n","    dH,dW, _ = (np.array(img2_color.shape) * bounding_large_tolerance).astype(int)\n","    bounding_box_with_large_tolerance = bounding_box + np.array([-dW,-dH,dW,dH])\n","\n","    orb_detector = cv2.SIFT_create(5000)\n","    img1_zero_background = np.zeros(img1.shape).astype(np.uint8)\n","    img1_zero_background[bounding_box_with_tolerance[1]:bounding_box_with_tolerance[3],\n","                    bounding_box_with_tolerance[0]:bounding_box_with_tolerance[2]\n","                    ] = img1[bounding_box_with_tolerance[1]:bounding_box_with_tolerance[3],\n","                    bounding_box_with_tolerance[0]:bounding_box_with_tolerance[2]\n","                    ]\n","    kp1, d1 = orb_detector.detectAndCompute(img1_zero_background, None)\n","    img2_zero_background = np.zeros(img2.shape).astype(np.uint8)\n","    img2_zero_background[bounding_box_with_large_tolerance[1]:bounding_box_with_large_tolerance[3],\n","                    bounding_box_with_large_tolerance[0]:bounding_box_with_large_tolerance[2]\n","                    ] = img2[bounding_box_with_large_tolerance[1]:bounding_box_with_large_tolerance[3],\n","                    bounding_box_with_large_tolerance[0]:bounding_box_with_large_tolerance[2]\n","                    ]\n","    kp2, d2 = orb_detector.detectAndCompute(img2_zero_background, None)\n","    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n","    if (d1 is None) or (d2 is None):\n","      matches = []\n","    else:\n","      matches_tuple = matcher.match(d1.astype(np.uint8), d2.astype(np.uint8))\n","      matches = list(matches_tuple)\n","      matches.sort(key=lambda x: x.distance)\n","    if len(matches) >= 4:  # Need at least 4 matches\n","      src_pts = np.float32([kp1[m.queryIdx].pt for m in matches[:2500]]).reshape(-1, 1, 2)\n","      dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches[:2500]]).reshape(-1, 1, 2)\n","\n","      H, _ = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5)\n","\n","      # Use the homography matrix to transform img2_color to align with img1_color\n","      img2_transformed = cv2.warpPerspective(img2_color, H, (img1_color.shape[1], img1_color.shape[0]))\n","\n","      # Isolate the patch area in img2_transformed using the original mask\n","      img2_patch = cv2.bitwise_and(img2_transformed, img2_transformed, mask=~mask)\n","\n","      # Combine the original img1_color with the patch from img2_transformed to fill in the masked out patch\n","      result = cv2.add(img1_color_with_patch_removed, img2_patch)\n","\n","    else:\n","      result = img1_color\n","\n","    plt.imsave(STICHED_IMAGES_PATH+f\"{idx}.jpg\", result)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155330,"status":"ok","timestamp":1714034995811,"user":{"displayName":"Pacome Simon Mbonimpa","userId":"08747643083832559810"},"user_tz":-120},"id":"Jm1v0242ZKTs","outputId":"a54d076b-fad9-41c3-8fea-1427165c2b74"},"outputs":[{"name":"stderr","output_type":"stream","text":["20it [02:34,  7.74s/it]\n"]}],"source":["generate_warped_images(STICHED_IMAGES_PATH = STICHED_IMAGES_PATH)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMAZiny6GSjUlknqGsRMIGp","mount_file_id":"1SMUcacyJTIZUmyjf6HZrvraMOA_l-AtW","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
